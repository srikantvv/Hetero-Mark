module &aes_opt_bc:1:0:$full:$large:$default;
extension "amd:gcn";
extension "IMAGE";
align(4) readonly_u8 &s[256] = u8[](99, 124, 119, 123, 242, 107, 111, 197, 48, 1, 103, 43, 254, 215, 171, 118, 202, 130, 201, 125, 250, 89, 71, 240, 173, 212, 162, 175, 156, 164, 114, 192, 183, 253, 147, 38, 54, 63, 247, 204, 52, 165, 229, 241, 113, 216, 49, 21, 4, 199, 35, 195, 24, 150, 5, 154, 7, 18, 128, 226, 235, 39, 178, 117, 9, 131, 44, 26, 27, 110, 90, 160, 82, 59, 214, 179, 41, 227, 47, 132, 83, 209, 0, 237, 32, 252, 177, 91, 106, 203, 190, 57, 74, 76, 88, 207, 208, 239, 170, 251, 67, 77, 51, 133, 69, 249, 2, 127, 80, 60, 159, 168, 81, 163, 64, 143, 146, 157, 56, 245, 188, 182, 218, 33, 16, 255, 243, 210, 205, 12, 19, 236, 95, 151, 68, 23, 196, 167, 126, 61, 100, 93, 25, 115, 96, 129, 79, 220, 34, 42, 144, 136, 70, 238, 184, 20, 222, 94, 11, 219, 224, 50, 58, 10, 73, 6, 36, 92, 194, 211, 172, 98, 145, 149, 228, 121, 231, 200, 55, 109, 141, 213, 78, 169, 108, 86, 244, 234, 101, 122, 174, 8, 186, 120, 37, 46, 28, 166, 180, 198, 232, 221, 116, 31, 75, 189, 139, 138, 112, 62, 181, 102, 72, 3, 246, 14, 97, 53, 87, 185, 134, 193, 29, 158, 225, 248, 152, 17, 105, 217, 142, 148, 155, 30, 135, 233, 206, 85, 40, 223, 140, 161, 137, 13, 191, 230, 66, 104, 65, 153, 45, 15, 176, 84, 187, 22);

decl prog function &abort()();

prog kernel &__OpenCL_Encrypt_kernel(
	kernarg_u64 %__global_offset_0,
	kernarg_u64 %__global_offset_1,
	kernarg_u64 %__global_offset_2,
	kernarg_u64 %__printf_buffer,
	kernarg_u64 %__vqueue_pointer,
	kernarg_u64 %__aqlwrap_pointer,
	kernarg_u64 %input,
	kernarg_u64 %expanded_key,
	kernarg_u64 %locPtr,
	kernarg_u64 %sigAddr)
{
	pragma "AMD RTI", "ARGSTART:__OpenCL_Encrypt_kernel";
	pragma "AMD RTI", "version:3:1:104";
	pragma "AMD RTI", "device:generic";
	pragma "AMD RTI", "uniqueid:1024";
	pragma "AMD RTI", "memory:private:16";
	pragma "AMD RTI", "memory:region:0";
	pragma "AMD RTI", "memory:local:0";
	pragma "AMD RTI", "value:__global_offset_0:u64:1:1:0";
	pragma "AMD RTI", "value:__global_offset_1:u64:1:1:16";
	pragma "AMD RTI", "value:__global_offset_2:u64:1:1:32";
	pragma "AMD RTI", "pointer:__printf_buffer:u8:1:1:48:uav:7:1:RW:0:0:0";
	pragma "AMD RTI", "value:__vqueue_pointer:u64:1:1:64";
	pragma "AMD RTI", "value:__aqlwrap_pointer:u64:1:1:80";
	pragma "AMD RTI", "pointer:input:u8:1:1:96:uav:7:1:RW:0:0:0";
	pragma "AMD RTI", "pointer:expanded_key:u32:1:1:112:uav:7:4:RW:0:0:0";
	pragma "AMD RTI", "pointer:locPtr:u32:1:1:128:uav:7:4:RW:0:0:0";
	pragma "AMD RTI", "value:sigAddr:u64:1:1:144";
	pragma "AMD RTI", "function:1:0";
	pragma "AMD RTI", "memory:64bitABI";
	pragma "AMD RTI", "privateid:8";
	pragma "AMD RTI", "enqueue_kernel:0";
	pragma "AMD RTI", "kernel_index:0";
	pragma "AMD RTI", "reflection:0:size_t";
	pragma "AMD RTI", "reflection:1:size_t";
	pragma "AMD RTI", "reflection:2:size_t";
	pragma "AMD RTI", "reflection:3:size_t";
	pragma "AMD RTI", "reflection:4:size_t";
	pragma "AMD RTI", "reflection:5:size_t";
	pragma "AMD RTI", "reflection:6:uchar*";
	pragma "AMD RTI", "reflection:7:uint*";
	pragma "AMD RTI", "reflection:8:int*";
	pragma "AMD RTI", "reflection:9:ulong";
	pragma "AMD RTI", "ARGEND:__OpenCL_Encrypt_kernel";
	align(8) spill_u8 %__spillStack[8];

@__OpenCL_Encrypt_kernel_entry:
	// BB#0:                                // %AddRoundKey.exit.preheader
	workitemabsid_u32	$s0, 0;
	cvt_u64_u32	$d0, $s0;
	ld_kernarg_align(8)_width(all)_u64	$d1, [%__global_offset_0];
	add_u64	$d0, $d0, $d1;
	cvt_u32_u64	$s0, $d0;
	shl_u32	$s2, $s0, 4;
	or_b32	$s5, $s2, 14;
	or_b32	$s6, $s2, 11;
	or_b32	$s7, $s2, 10;
	or_b32	$s1, $s2, 15;
	or_b32	$s3, $s2, 13;
	or_b32	$s4, $s2, 12;
	cvt_s64_s32	$d0, $s2;
	ld_kernarg_align(8)_width(all)_u64	$d16, [%input];
	cvt_s64_s32	$d2, $s7;
	cvt_s64_s32	$d3, $s6;
	cvt_s64_s32	$d1, $s5;
	ld_kernarg_align(8)_width(all)_u64	$d7, [%expanded_key];
	or_b32	$s6, $s2, 9;
	or_b32	$s7, $s2, 6;
	or_b32	$s9, $s2, 1;
	add_u64	$d8, $d16, $d0;
	cvt_s64_s32	$d6, $s4;
	cvt_s64_s32	$d5, $s3;
	cvt_s64_s32	$d9, $s1;
	ld_kernarg_align(8)_width(all)_u64	$d0, [%locPtr];
	or_b32	$s8, $s2, 3;
	or_b32	$s10, $s2, 2;
	add_u64	$d1, $d16, $d1;
	or_b32	$s13, $s2, 8;
	gridsize_u32	$s1, 0;
	add_u64	$d3, $d16, $d3;
	add_u64	$d4, $d16, $d2;
	add_u64	$d2, $d16, $d9;
	or_b32	$s15, $s2, 7;
	add_u64	$d5, $d16, $d5;
	or_b32	$s11, $s2, 5;
	or_b32	$s14, $s2, 4;
	ld_v4_global_align(4)_const_width(all)_u32	($s16, $s12, $s3, $s4), [$d7];
	mov_b32	$s2, 13;
	add_u64	$d6, $d16, $d6;
	ld_global_u8	$s5, [$d8];
	cvt_s64_s32	$d9, $s9;
	add_u64	$d17, $d7, 16;
	cvt_s64_s32	$d20, $s7;
	cvt_s64_s32	$d13, $s6;
	ld_global_u8	$s7, [$d6];
	cvt_s64_s32	$d12, $s13;
	ld_global_u8	$s6, [$d5];
	cvt_s64_s32	$d11, $s15;
	ld_global_u8	$s9, [$d1];
	cvt_s64_s32	$d18, $s10;
	ld_global_u8	$s13, [$d2];
	cvt_s64_s32	$d15, $s8;
	ld_global_u8	$s8, [$d4];
	cvt_s64_s32	$d10, $s14;
	ld_global_u8	$s10, [$d3];
	add_u64	$d19, $d16, $d9;
	ld_global_u8	$s18, [$d19];
	cvt_s64_s32	$d9, $s11;
	cvt_u64_u32	$d14, $s1;
	st_spill_align(8)_u64	$d14, [%__spillStack];
	add_u64	$d9, $d16, $d9;
	add_u64	$d10, $d16, $d10;
	add_u64	$d15, $d16, $d15;
	add_u64	$d18, $d16, $d18;
	add_u64	$d11, $d16, $d11;
	add_u64	$d12, $d16, $d12;
	add_u64	$d13, $d16, $d13;
	add_u64	$d16, $d16, $d20;
	shr_u32	$s23, $s4, 16;
	shr_u32	$s25, $s4, 8;
	shr_u32	$s11, $s16, 24;
	shr_u32	$s21, $s3, 8;
	shr_u32	$s26, $s4, 24;
	shr_u32	$s19, $s16, 16;
	shr_u32	$s17, $s16, 8;
	shr_u32	$s15, $s12, 24;
	shr_u32	$s14, $s12, 16;
	shr_u32	$s20, $s12, 8;
	shr_u32	$s24, $s3, 24;
	xor_b32	$s11, $s5, $s11;
	shr_u32	$s27, $s3, 16;
	xor_b32	$s4, $s13, $s4;
	ld_global_u8	$s22, [$d18];
	xor_b32	$s5, $s9, $s25;
	ld_global_u8	$s25, [$d15];
	xor_b32	$s6, $s6, $s23;
	ld_global_u8	$s23, [$d10];
	xor_b32	$s7, $s7, $s26;
	ld_global_u8	$s26, [$d9];
	xor_b32	$s3, $s10, $s3;
	ld_global_u8	$s13, [$d16];
	xor_b32	$s8, $s8, $s21;
	ld_global_u8	$s9, [$d13];
	xor_b32	$s9, $s9, $s27;
	ld_global_u8	$s10, [$d12];
	xor_b32	$s10, $s10, $s24;
	ld_global_u8	$s21, [$d11];
	xor_b32	$s12, $s21, $s12;
	xor_b32	$s13, $s13, $s20;
	xor_b32	$s14, $s26, $s14;
	xor_b32	$s15, $s23, $s15;
	xor_b32	$s16, $s25, $s16;
	xor_b32	$s17, $s22, $s17;
	xor_b32	$s18, $s18, $s19;
	br	@BB0_1;

@BB0_7:
	// %.preheader9
	and_b32	$s3, $s18, 255;
	shl_u32	$s3, $s3, 1;
	xor_b32	$s4, $s3, 27;
	and_b32	$s16, $s18, 128;
	cmp_eq_b1_s32	$c0, $s16, 0;
	cmov_b32	$s35, $c0, $s3, $s4;
	and_b32	$s3, $s15, 255;
	shl_u32	$s3, $s3, 1;
	xor_b32	$s4, $s3, 27;
	and_b32	$s16, $s15, 128;
	cmp_eq_b1_s32	$c0, $s16, 0;
	cmov_b32	$s36, $c0, $s3, $s4;
	and_b32	$s3, $s22, 255;
	shl_u32	$s3, $s3, 1;
	xor_b32	$s4, $s3, 27;
	and_b32	$s16, $s22, 128;
	cmp_eq_b1_s32	$c0, $s16, 0;
	cmov_b32	$s37, $c0, $s3, $s4;
	and_b32	$s3, $s14, 255;
	shl_u32	$s3, $s3, 1;
	xor_b32	$s4, $s3, 27;
	and_b32	$s16, $s14, 128;
	cmp_eq_b1_s32	$c0, $s16, 0;
	cmov_b32	$s38, $c0, $s3, $s4;
	xor_b32	$s3, $s19, $s36;
	xor_b32	$s4, $s3, $s11;
	ld_v4_global_align(4)_const_width(all)_u32	($s16, $s20, $s23, $s3), [$d17];
	xor_b32	$s4, $s4, $s35;
	xor_b32	$s4, $s4, $s18;
	shr_u32	$s27, $s23, 8;
	shr_u32	$s32, $s3, 8;
	shr_u32	$s28, $s23, 16;
	shr_u32	$s31, $s3, 24;
	shr_u32	$s33, $s3, 16;
	and_b32	$s24, $s4, 255;
	xor_b32	$s4, $s17, $s38;
	xor_b32	$s4, $s4, $s21;
	xor_b32	$s4, $s4, $s37;
	xor_b32	$s4, $s4, $s22;
	and_b32	$s4, $s4, 255;
	xor_b32	$s4, $s4, $s3;
	xor_b32	$s3, $s24, $s23;
	shr_u32	$s34, $s23, 24;
	shr_u32	$s23, $s16, 24;
	shr_u32	$s24, $s16, 16;
	shr_u32	$s25, $s16, 8;
	shr_u32	$s26, $s20, 24;
	shr_u32	$s29, $s20, 16;
	shr_u32	$s30, $s20, 8;
	and_b32	$s39, $s17, 128;
	cmp_eq_b1_s32	$c0, $s39, 0;
	and_b32	$s39, $s17, 255;
	shl_u32	$s39, $s39, 1;
	xor_b32	$s40, $s39, 27;
	cmov_b32	$s41, $c0, $s39, $s40;
	and_b32	$s39, $s11, 128;
	cmp_eq_b1_s32	$c0, $s39, 0;
	and_b32	$s39, $s11, 255;
	shl_u32	$s39, $s39, 1;
	xor_b32	$s40, $s39, 27;
	cmov_b32	$s39, $c0, $s39, $s40;
	and_b32	$s40, $s19, 128;
	cmp_eq_b1_s32	$c0, $s40, 0;
	and_b32	$s40, $s19, 255;
	shl_u32	$s40, $s40, 1;
	xor_b32	$s42, $s40, 27;
	cmov_b32	$s42, $c0, $s40, $s42;
	and_b32	$s40, $s21, 128;
	cmp_eq_b1_s32	$c0, $s40, 0;
	and_b32	$s40, $s21, 255;
	shl_u32	$s40, $s40, 1;
	xor_b32	$s43, $s40, 27;
	cmov_b32	$s47, $c0, $s40, $s43;
	and_b32	$s40, $s12, 128;
	cmp_eq_b1_s32	$c0, $s40, 0;
	and_b32	$s40, $s12, 255;
	shl_u32	$s40, $s40, 1;
	xor_b32	$s43, $s40, 27;
	cmov_b32	$s40, $c0, $s40, $s43;
	and_b32	$s43, $s13, 128;
	cmp_eq_b1_s32	$c0, $s43, 0;
	and_b32	$s43, $s13, 255;
	shl_u32	$s43, $s43, 1;
	xor_b32	$s44, $s43, 27;
	cmov_b32	$s44, $c0, $s43, $s44;
	and_b32	$s43, $s6, 128;
	cmp_eq_b1_s32	$c0, $s43, 0;
	and_b32	$s43, $s6, 255;
	shl_u32	$s43, $s43, 1;
	xor_b32	$s45, $s43, 27;
	cmov_b32	$s43, $c0, $s43, $s45;
	and_b32	$s45, $s10, 128;
	cmp_eq_b1_s32	$c0, $s45, 0;
	and_b32	$s45, $s10, 255;
	shl_u32	$s45, $s45, 1;
	xor_b32	$s46, $s45, 27;
	cmov_b32	$s46, $c0, $s45, $s46;
	and_b32	$s45, $s9, 128;
	cmp_eq_b1_s32	$c0, $s45, 0;
	and_b32	$s45, $s9, 255;
	shl_u32	$s45, $s45, 1;
	xor_b32	$s48, $s45, 27;
	cmov_b32	$s45, $c0, $s45, $s48;
	and_b32	$s48, $s5, 128;
	cmp_eq_b1_s32	$c0, $s48, 0;
	and_b32	$s48, $s5, 255;
	shl_u32	$s48, $s48, 1;
	xor_b32	$s49, $s48, 27;
	cmov_b32	$s48, $c0, $s48, $s49;
	and_b32	$s49, $s7, 128;
	cmp_eq_b1_s32	$c0, $s49, 0;
	and_b32	$s49, $s7, 255;
	shl_u32	$s49, $s49, 1;
	xor_b32	$s50, $s49, 27;
	cmov_b32	$s50, $c0, $s49, $s50;
	and_b32	$s49, $s8, 128;
	cmp_eq_b1_s32	$c0, $s49, 0;
	and_b32	$s49, $s8, 255;
	shl_u32	$s49, $s49, 1;
	xor_b32	$s51, $s49, 27;
	cmov_b32	$s49, $c0, $s49, $s51;
	xor_b32	$s51, $s21, $s41;
	xor_b32	$s51, $s51, $s22;
	xor_b32	$s38, $s51, $s38;
	xor_b32	$s22, $s22, $s47;
	xor_b32	$s22, $s22, $s14;
	xor_b32	$s22, $s22, $s41;
	xor_b32	$s37, $s14, $s37;
	xor_b32	$s37, $s37, $s17;
	xor_b32	$s37, $s37, $s47;
	xor_b32	$s41, $s11, $s42;
	xor_b32	$s41, $s41, $s18;
	xor_b32	$s36, $s41, $s36;
	xor_b32	$s18, $s18, $s39;
	xor_b32	$s18, $s18, $s15;
	xor_b32	$s18, $s18, $s42;
	xor_b32	$s35, $s15, $s35;
	xor_b32	$s35, $s35, $s19;
	xor_b32	$s35, $s35, $s39;
	xor_b32	$s11, $s35, $s11;
	xor_b32	$s18, $s18, $s19;
	xor_b32	$s15, $s36, $s15;
	xor_b32	$s19, $s37, $s21;
	xor_b32	$s17, $s22, $s17;
	xor_b32	$s14, $s38, $s14;
	xor_b32	$s21, $s10, $s43;
	xor_b32	$s21, $s21, $s12;
	xor_b32	$s35, $s21, $s44;
	xor_b32	$s21, $s6, $s44;
	xor_b32	$s21, $s21, $s10;
	xor_b32	$s36, $s21, $s40;
	xor_b32	$s21, $s13, $s40;
	xor_b32	$s21, $s21, $s6;
	xor_b32	$s39, $s21, $s46;
	xor_b32	$s21, $s12, $s46;
	xor_b32	$s21, $s21, $s13;
	xor_b32	$s40, $s21, $s43;
	xor_b32	$s21, $s9, $s49;
	xor_b32	$s21, $s21, $s5;
	xor_b32	$s38, $s21, $s50;
	xor_b32	$s21, $s8, $s50;
	xor_b32	$s21, $s21, $s9;
	xor_b32	$s37, $s21, $s48;
	xor_b32	$s21, $s7, $s48;
	xor_b32	$s21, $s21, $s8;
	xor_b32	$s22, $s21, $s45;
	xor_b32	$s21, $s5, $s45;
	xor_b32	$s21, $s21, $s7;
	xor_b32	$s21, $s21, $s49;
	xor_b32	$s21, $s21, $s8;
	xor_b32	$s22, $s22, $s9;
	xor_b32	$s37, $s37, $s5;
	xor_b32	$s38, $s38, $s7;
	xor_b32	$s13, $s35, $s13;
	xor_b32	$s35, $s40, $s6;
	xor_b32	$s39, $s39, $s10;
	xor_b32	$s12, $s36, $s12;
	and_b32	$s5, $s14, 255;
	xor_b32	$s5, $s5, $s32;
	and_b32	$s6, $s17, 255;
	xor_b32	$s6, $s6, $s33;
	and_b32	$s7, $s19, 255;
	xor_b32	$s7, $s7, $s31;
	and_b32	$s8, $s15, 255;
	xor_b32	$s8, $s8, $s27;
	and_b32	$s9, $s18, 255;
	xor_b32	$s9, $s9, $s28;
	and_b32	$s10, $s11, 255;
	xor_b32	$s10, $s10, $s34;
	add_u32	$s2, $s2, 4294967295;
	add_u64	$d17, $d17, 16;
	and_b32	$s11, $s12, 255;
	and_b32	$s18, $s39, 255;
	and_b32	$s17, $s35, 255;
	and_b32	$s19, $s13, 255;
	and_b32	$s15, $s38, 255;
	and_b32	$s14, $s37, 255;
	and_b32	$s13, $s22, 255;
	and_b32	$s12, $s21, 255;
	xor_b32	$s12, $s12, $s20;
	xor_b32	$s13, $s13, $s30;
	xor_b32	$s14, $s14, $s29;
	xor_b32	$s15, $s15, $s26;
	xor_b32	$s16, $s19, $s16;
	xor_b32	$s17, $s17, $s25;
	xor_b32	$s18, $s18, $s24;
	xor_b32	$s11, $s11, $s23;

@BB0_1:
	// %AddRoundKey.exit
	and_b32	$s19, $s4, 255;
	and_b32	$s5, $s5, 255;
	and_b32	$s6, $s6, 255;
	and_b32	$s4, $s15, 255;
	and_b32	$s15, $s16, 255;
	and_b32	$s16, $s17, 255;
	and_b32	$s17, $s18, 255;
	and_b32	$s18, $s11, 255;
	cvt_u64_u32	$d20, $s6;
	cvt_u64_u32	$d21, $s5;
	cvt_u64_u32	$d22, $s19;
	and_b32	$s8, $s8, 255;
	and_b32	$s9, $s9, 255;
	and_b32	$s10, $s10, 255;
	and_b32	$s12, $s12, 255;
	and_b32	$s13, $s13, 255;
	and_b32	$s14, $s14, 255;
	ld_readonly_u8	$s6, [&s][$d22];
	ld_readonly_u8	$s5, [&s][$d21];
	ld_readonly_u8	$s11, [&s][$d20];
	cvt_u64_u32	$d20, $s18;
	cvt_u64_u32	$d21, $s17;
	cvt_u64_u32	$d22, $s16;
	cvt_u64_u32	$d23, $s15;
	cvt_u64_u32	$d24, $s4;
	cvt_u64_u32	$d25, $s14;
	cvt_u64_u32	$d26, $s13;
	cvt_u64_u32	$d27, $s12;
	cvt_u64_u32	$d28, $s10;
	cvt_u64_u32	$d29, $s9;
	cvt_u64_u32	$d30, $s8;
	and_b32	$s3, $s3, 255;
	cvt_u64_u32	$d31, $s3;
	and_b32	$s3, $s7, 255;
	cvt_u64_u32	$d14, $s3;
	ld_readonly_u8	$s22, [&s][$d14];
	ld_readonly_u8	$s14, [&s][$d31];
	ld_readonly_u8	$s10, [&s][$d30];
	ld_readonly_u8	$s7, [&s][$d29];
	ld_readonly_u8	$s18, [&s][$d28];
	ld_readonly_u8	$s15, [&s][$d27];
	ld_readonly_u8	$s17, [&s][$d26];
	ld_readonly_u8	$s12, [&s][$d25];
	ld_readonly_u8	$s8, [&s][$d24];
	ld_readonly_u8	$s9, [&s][$d23];
	ld_readonly_u8	$s19, [&s][$d22];
	ld_readonly_u8	$s21, [&s][$d21];
	ld_readonly_u8	$s13, [&s][$d20];
	cmp_ne_b1_s32	$c0, $s2, 0;
	cbr_b1	$c0, @BB0_7;
	// BB#2:                                // %.preheader7
	ld_v4_global_align(4)_const_width(all)_u32	($s4, $s2, $s16, $s3), [$d7+224];
	shr_u32	$s20, $s4, 24;
	xor_b32	$s13, $s13, $s20;
	st_global_u8	$s13, [$d8];
	shr_u32	$s13, $s4, 16;
	xor_b32	$s12, $s12, $s13;
	shr_u32	$s13, $s4, 8;
	st_global_u8	$s12, [$d19];
	xor_b32	$s12, $s10, $s13;
	shr_u32	$s13, $s3, 8;
	shr_u32	$s10, $s3, 24;
	shr_u32	$s26, $s3, 16;
	shr_u32	$s20, $s16, 24;
	shr_u32	$s24, $s16, 16;
	shr_u32	$s27, $s16, 8;
	st_global_u8	$s12, [$d18];
	xor_b32	$s12, $s6, $s4;
	shr_u32	$s23, $s2, 24;
	shr_u32	$s25, $s2, 16;
	shr_u32	$s28, $s2, 8;
	xor_b32	$s3, $s14, $s3;
	xor_b32	$s4, $s17, $s13;
	xor_b32	$s6, $s21, $s26;
	xor_b32	$s10, $s22, $s10;
	xor_b32	$s13, $s15, $s16;
	xor_b32	$s14, $s19, $s27;
	xor_b32	$s11, $s11, $s24;
	xor_b32	$s15, $s18, $s20;
	xor_b32	$s2, $s9, $s2;
	xor_b32	$s5, $s5, $s28;
	xor_b32	$s7, $s7, $s25;
	xor_b32	$s8, $s8, $s23;
	ld_spill_align(8)_u64	$d7, [%__spillStack];
	add_u64	$d7, $d7, 18446744073709551615;
	cvt_s64_s32	$d8, $s0;
	mov_b32	$s0, 1;
	st_global_u8	$s12, [$d15];
	st_global_u8	$s8, [$d10];
	st_global_u8	$s7, [$d9];
	st_global_u8	$s5, [$d16];
	st_global_u8	$s2, [$d11];
	st_global_u8	$s15, [$d12];
	st_global_u8	$s11, [$d13];
	st_global_u8	$s14, [$d4];
	st_global_u8	$s13, [$d3];
	st_global_u8	$s10, [$d6];
	st_global_u8	$s6, [$d5];
	st_global_u8	$s4, [$d1];
	st_global_u8	$s3, [$d2];
	atomicnoret_add_global_scar_agent_s32	[$d0], $s0;
	cmp_ne_b1_s64	$c0, $d8, $d7;
	cbr_b1	$c0, @BB0_6;
	// BB#3:                                // %.preheader

@BB0_4:
	ld_global_align(4)_width(all)_u32	$s0, [$d0];
	cmp_ne_b1_s32	$c0, $s0, $s1;
	cbr_b1	$c0, @BB0_4;
	// BB#5:
	ld_kernarg_align(8)_width(all)_u64	$d0, [%sigAddr];
	st_global_align(8)_u64	1, [$d0];

@BB0_6:
	ret;
};
